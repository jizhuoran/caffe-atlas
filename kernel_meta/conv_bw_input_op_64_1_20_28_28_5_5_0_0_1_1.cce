#ifdef __CCE_KT_TEST__
#define __aicore__ 
#else
#define __aicore__ [aicore]
#endif


#define VERIFY_L2Buffer_OK(l2DataIndex) \
	(0x01 & (((0xFF)&(~l2_in_main)) >> (l2DataIndex)))

extern "C"  __global__ __aicore__ void conv_bw_input_op_64_1_20_28_28_5_5_0_0_1_1__kernel0(__gm__ half* __restrict__ weight_five, __gm__ half* __restrict__ out_diff, __gm__ half* __restrict__ c_ddr,int64_t index0, uint64_t offset0, int64_t index1, uint64_t offset1, int64_t index2, uint64_t offset2) {
  if (index0 >= 0) {
    if (VERIFY_L2Buffer_OK(index0)) {
      weight_five = (__gm__ half*)((uint64_t)l2_vaddr_base + offset0);
    }
  }
  if (index1 >= 0) {
    if (VERIFY_L2Buffer_OK(index1)) {
      out_diff = (__gm__ half*)((uint64_t)l2_vaddr_base + offset1);
    }
  }
  if (index2 >= 0) {
    if (VERIFY_L2Buffer_OK(index2)) {
      c_ddr = (__gm__ half*)((uint64_t)l2_vaddr_base + offset2);
    }
  }
set_l1_3d_size(0);
set_padding(0);
__cbuf__   half* weight_five_local_L1 = (__cbuf__  half *)get_imm(0);
__cb__   half* w_col = (__cb__  half *)get_imm(0);
__cbuf__   half* out_diff_local_L1_1 = (__cbuf__  half *)get_imm(25600);
__cc__   float* C_2 = (__cc__  float *)get_imm(0);
__ca__   half* im2col_fractal_4 = (__ca__  half *)get_imm(0);
__ca__   half* im2col_fractal_5 = (__ca__  half *)get_imm(17920);
__ubuf__   half* c_ub_3 = (__ubuf__  half *)get_imm(0);
__cc__   float* C_6 = (__cc__  float *)get_imm(7168);
__ca__   half* im2col_fractal_8 = (__ca__  half *)get_imm(35840);
__ubuf__   half* c_ub_7 = (__ubuf__  half *)get_imm(3584);
__ubuf__   half* c_ub_11 = (__ubuf__  half *)get_imm(7168);
__cbuf__   half* out_diff_local_L1_14 = (__cbuf__  half *)get_imm(62464);
  set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
  set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
  set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
  set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
  set_fmatrix((uint64_t)289360691286507544);
  set_padding((uint64_t)0);
  copy_gm_to_cbuf(((__cbuf__ half *)weight_five_local_L1 + 0), ((__gm__ half *)weight_five + 0), 0, 1, 800, 0, 0, PAD_NONE);
  set_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
  for (int32_t w_k1_idx = 0; w_k1_idx < 50; ++w_k1_idx) {
    load_cbuf_to_cb(((__cb__ half *)w_col + (w_k1_idx * 256)), ((__cbuf__ half *)weight_five_local_L1 + ((((w_k1_idx / 25) * 256) + 12288) - ((w_k1_idx % 25) * 512))), 0, 1, 0, 0, 1);
  }
  for (int32_t dx_batch_idx_outer_inner_db = 0; dx_batch_idx_outer_inner_db < 32; ++dx_batch_idx_outer_inner_db) {
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    set_flag(PIPE_V, PIPE_M, EVENT_ID1);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    set_flag(PIPE_V, PIPE_M, EVENT_ID0);
    set_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
    copy_gm_to_cbuf(((__cbuf__ half *)out_diff_local_L1_1 + 0), ((__gm__ half *)out_diff + (dx_batch_idx_outer_inner_db * 36864)), 0, 1, 1152, 0, 0, PAD_NONE);
    set_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
    wait_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
    for (int32_t dx_hw_idx_outer_inner_db = 0; dx_hw_idx_outer_inner_db < 3; ++dx_hw_idx_outer_inner_db) {
      wait_flag(PIPE_V, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
      pipe_barrier(PIPE_MTE1);
      for (int32_t axis_k1_outer_db = 0; axis_k1_outer_db < 5; ++axis_k1_outer_db) {
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        for (int32_t lower = 0; lower < 5; ++lower) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_4 + (lower * 256)), ((__cbuf__ half *)out_diff_local_L1_1 + 0), ((uint64_t)((((((int64_t)(axis_k1_outer_db * 2)) * (int64_t)5) + ((int64_t)lower)) - ((int64_t)(((uint64_t)(((((int64_t)(axis_k1_outer_db * 2)) * (int64_t)5) + ((int64_t)lower)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)(axis_k1_outer_db * 2)) * (int64_t)5) + ((int64_t)lower)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)(dx_hw_idx_outer_inner_db * 2)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)(axis_k1_outer_db * 2)) * (int64_t)5) + ((int64_t)lower)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_2 + 0), ((__ca__ half *)im2col_fractal_4 + 0), ((__cb__ half *)w_col + (axis_k1_outer_db * 2560)), 112, 80, 16, (axis_k1_outer_db == 0));
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
        for (int32_t lower1 = 0; lower1 < 5; ++lower1) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_5 + (lower1 * 256)), ((__cbuf__ half *)out_diff_local_L1_1 + 0), ((uint64_t)((((((int64_t)((axis_k1_outer_db * 2) + 1)) * (int64_t)5) + ((int64_t)lower1)) - ((int64_t)(((uint64_t)(((((int64_t)((axis_k1_outer_db * 2) + 1)) * (int64_t)5) + ((int64_t)lower1)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)((axis_k1_outer_db * 2) + 1)) * (int64_t)5) + ((int64_t)lower1)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)(dx_hw_idx_outer_inner_db * 2)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)((axis_k1_outer_db * 2) + 1)) * (int64_t)5) + ((int64_t)lower1)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_2 + 0), ((__ca__ half *)im2col_fractal_5 + 0), ((__cb__ half *)w_col + ((axis_k1_outer_db * 2560) + 1280)), 112, 80, 16, (int8_t)0);
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
      }
      set_flag(PIPE_M, PIPE_V, EVENT_ID0);
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
      wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
      copy_matrix_cc_to_ubuf(((__ubuf__ half *)c_ub_3 + 0), ((__cc__ float *)C_2 + 0), 0, 1, 7, 0, 0, CRMODE_F32toF16_NONE);
      set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      set_flag(PIPE_V, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      copy_ubuf_to_gm(((__gm__ half *)c_ddr + ((dx_batch_idx_outer_inner_db * 25088) + (dx_hw_idx_outer_inner_db * 3584))), ((__ubuf__ half *)c_ub_3 + 0), 0, 1, 112, 0, 0);
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
      wait_flag(PIPE_V, PIPE_M, EVENT_ID1);
      for (int32_t axis_k1_outer_db1 = 0; axis_k1_outer_db1 < 5; ++axis_k1_outer_db1) {
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        for (int32_t lower2 = 0; lower2 < 5; ++lower2) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_8 + (lower2 * 256)), ((__cbuf__ half *)out_diff_local_L1_1 + 0), ((uint64_t)((((((int64_t)(axis_k1_outer_db1 * 2)) * (int64_t)5) + ((int64_t)lower2)) - ((int64_t)(((uint64_t)(((((int64_t)(axis_k1_outer_db1 * 2)) * (int64_t)5) + ((int64_t)lower2)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)(axis_k1_outer_db1 * 2)) * (int64_t)5) + ((int64_t)lower2)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)((dx_hw_idx_outer_inner_db * 2) + 1)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)(axis_k1_outer_db1 * 2)) * (int64_t)5) + ((int64_t)lower2)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_6 + 0), ((__ca__ half *)im2col_fractal_8 + 0), ((__cb__ half *)w_col + (axis_k1_outer_db1 * 2560)), 112, 80, 16, (axis_k1_outer_db1 == 0));
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
        for (int32_t lower3 = 0; lower3 < 5; ++lower3) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_4 + (lower3 * 256)), ((__cbuf__ half *)out_diff_local_L1_1 + 0), ((uint64_t)((((((int64_t)((axis_k1_outer_db1 * 2) + 1)) * (int64_t)5) + ((int64_t)lower3)) - ((int64_t)(((uint64_t)(((((int64_t)((axis_k1_outer_db1 * 2) + 1)) * (int64_t)5) + ((int64_t)lower3)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)((axis_k1_outer_db1 * 2) + 1)) * (int64_t)5) + ((int64_t)lower3)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)((dx_hw_idx_outer_inner_db * 2) + 1)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)((axis_k1_outer_db1 * 2) + 1)) * (int64_t)5) + ((int64_t)lower3)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_6 + 0), ((__ca__ half *)im2col_fractal_4 + 0), ((__cb__ half *)w_col + ((axis_k1_outer_db1 * 2560) + 1280)), 112, 80, 16, (int8_t)0);
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
      }
      set_flag(PIPE_M, PIPE_V, EVENT_ID0);
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
      wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
      copy_matrix_cc_to_ubuf(((__ubuf__ half *)c_ub_7 + 0), ((__cc__ float *)C_6 + 0), 0, 1, 7, 0, 0, CRMODE_F32toF16_NONE);
      set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      set_flag(PIPE_V, PIPE_M, EVENT_ID1);
      wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      copy_ubuf_to_gm(((__gm__ half *)c_ddr + (((dx_batch_idx_outer_inner_db * 25088) + (dx_hw_idx_outer_inner_db * 3584)) + 1792)), ((__ubuf__ half *)c_ub_7 + 0), 0, 1, 112, 0, 0);
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    }
    wait_flag(PIPE_V, PIPE_M, EVENT_ID1);
    wait_flag(PIPE_V, PIPE_M, EVENT_ID0);
    wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
    for (int32_t axis_k1_outer_db2 = 0; axis_k1_outer_db2 < 5; ++axis_k1_outer_db2) {
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      for (int32_t lower4 = 0; lower4 < 5; ++lower4) {
        pipe_barrier(PIPE_MTE1);
        img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_5 + (lower4 * 256)), ((__cbuf__ half *)out_diff_local_L1_1 + 0), ((uint64_t)((((((int64_t)(axis_k1_outer_db2 * 2)) * (int64_t)5) + ((int64_t)lower4)) - ((int64_t)(((uint64_t)(((((int64_t)(axis_k1_outer_db2 * 2)) * (int64_t)5) + ((int64_t)lower4)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)(axis_k1_outer_db2 * 2)) * (int64_t)5) + ((int64_t)lower4)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, (int64_t)20, ((uint64_t)(((((int64_t)(axis_k1_outer_db2 * 2)) * (int64_t)5) + ((int64_t)lower4)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
      }
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      pipe_barrier(PIPE_M);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)C_2 + 0), ((__ca__ half *)im2col_fractal_5 + 0), ((__cb__ half *)w_col + (axis_k1_outer_db2 * 2560)), 112, 80, 16, (axis_k1_outer_db2 == 0));
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
      for (int32_t lower5 = 0; lower5 < 5; ++lower5) {
        pipe_barrier(PIPE_MTE1);
        img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_8 + (lower5 * 256)), ((__cbuf__ half *)out_diff_local_L1_1 + 0), ((uint64_t)((((((int64_t)((axis_k1_outer_db2 * 2) + 1)) * (int64_t)5) + ((int64_t)lower5)) - ((int64_t)(((uint64_t)(((((int64_t)((axis_k1_outer_db2 * 2) + 1)) * (int64_t)5) + ((int64_t)lower5)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)((axis_k1_outer_db2 * 2) + 1)) * (int64_t)5) + ((int64_t)lower5)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, (int64_t)20, ((uint64_t)(((((int64_t)((axis_k1_outer_db2 * 2) + 1)) * (int64_t)5) + ((int64_t)lower5)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
      }
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      pipe_barrier(PIPE_M);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)C_2 + 0), ((__ca__ half *)im2col_fractal_8 + 0), ((__cb__ half *)w_col + ((axis_k1_outer_db2 * 2560) + 1280)), 112, 80, 16, (int8_t)0);
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
    }
    set_flag(PIPE_M, PIPE_V, EVENT_ID0);
    set_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
    set_flag(PIPE_V, PIPE_M, EVENT_ID0);
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
    wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
    copy_matrix_cc_to_ubuf(((__ubuf__ half *)c_ub_11 + 0), ((__cc__ float *)C_2 + 0), 0, 1, 7, 0, 0, CRMODE_F32toF16_NONE);
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    set_flag(PIPE_V, PIPE_M, EVENT_ID1);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)c_ddr + ((dx_batch_idx_outer_inner_db * 25088) + 10752)), ((__ubuf__ half *)c_ub_11 + 0), 0, 1, 112, 0, 0);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
    copy_gm_to_cbuf(((__cbuf__ half *)out_diff_local_L1_14 + 0), ((__gm__ half *)out_diff + ((dx_batch_idx_outer_inner_db * 36864) + 18432)), 0, 1, 1152, 0, 0, PAD_NONE);
    set_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
    wait_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
    for (int32_t dx_hw_idx_outer_inner_db1 = 0; dx_hw_idx_outer_inner_db1 < 3; ++dx_hw_idx_outer_inner_db1) {
      wait_flag(PIPE_V, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
      pipe_barrier(PIPE_MTE1);
      for (int32_t axis_k1_outer_db3 = 0; axis_k1_outer_db3 < 5; ++axis_k1_outer_db3) {
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        for (int32_t lower6 = 0; lower6 < 5; ++lower6) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_4 + (lower6 * 256)), ((__cbuf__ half *)out_diff_local_L1_14 + 0), ((uint64_t)((((((int64_t)(axis_k1_outer_db3 * 2)) * (int64_t)5) + ((int64_t)lower6)) - ((int64_t)(((uint64_t)(((((int64_t)(axis_k1_outer_db3 * 2)) * (int64_t)5) + ((int64_t)lower6)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)(axis_k1_outer_db3 * 2)) * (int64_t)5) + ((int64_t)lower6)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)(dx_hw_idx_outer_inner_db1 * 2)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)(axis_k1_outer_db3 * 2)) * (int64_t)5) + ((int64_t)lower6)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_6 + 0), ((__ca__ half *)im2col_fractal_4 + 0), ((__cb__ half *)w_col + (axis_k1_outer_db3 * 2560)), 112, 80, 16, (axis_k1_outer_db3 == 0));
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
        for (int32_t lower7 = 0; lower7 < 5; ++lower7) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_5 + (lower7 * 256)), ((__cbuf__ half *)out_diff_local_L1_14 + 0), ((uint64_t)((((((int64_t)((axis_k1_outer_db3 * 2) + 1)) * (int64_t)5) + ((int64_t)lower7)) - ((int64_t)(((uint64_t)(((((int64_t)((axis_k1_outer_db3 * 2) + 1)) * (int64_t)5) + ((int64_t)lower7)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)((axis_k1_outer_db3 * 2) + 1)) * (int64_t)5) + ((int64_t)lower7)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)(dx_hw_idx_outer_inner_db1 * 2)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)((axis_k1_outer_db3 * 2) + 1)) * (int64_t)5) + ((int64_t)lower7)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_6 + 0), ((__ca__ half *)im2col_fractal_5 + 0), ((__cb__ half *)w_col + ((axis_k1_outer_db3 * 2560) + 1280)), 112, 80, 16, (int8_t)0);
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
      }
      set_flag(PIPE_M, PIPE_V, EVENT_ID0);
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
      wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
      copy_matrix_cc_to_ubuf(((__ubuf__ half *)c_ub_3 + 0), ((__cc__ float *)C_6 + 0), 0, 1, 7, 0, 0, CRMODE_F32toF16_NONE);
      set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      set_flag(PIPE_V, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      copy_ubuf_to_gm(((__gm__ half *)c_ddr + (((dx_batch_idx_outer_inner_db * 25088) + (dx_hw_idx_outer_inner_db1 * 3584)) + 12544)), ((__ubuf__ half *)c_ub_3 + 0), 0, 1, 112, 0, 0);
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
      wait_flag(PIPE_V, PIPE_M, EVENT_ID1);
      for (int32_t axis_k1_outer_db4 = 0; axis_k1_outer_db4 < 5; ++axis_k1_outer_db4) {
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        for (int32_t lower8 = 0; lower8 < 5; ++lower8) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_8 + (lower8 * 256)), ((__cbuf__ half *)out_diff_local_L1_14 + 0), ((uint64_t)((((((int64_t)(axis_k1_outer_db4 * 2)) * (int64_t)5) + ((int64_t)lower8)) - ((int64_t)(((uint64_t)(((((int64_t)(axis_k1_outer_db4 * 2)) * (int64_t)5) + ((int64_t)lower8)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)(axis_k1_outer_db4 * 2)) * (int64_t)5) + ((int64_t)lower8)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)((dx_hw_idx_outer_inner_db1 * 2) + 1)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)(axis_k1_outer_db4 * 2)) * (int64_t)5) + ((int64_t)lower8)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_2 + 0), ((__ca__ half *)im2col_fractal_8 + 0), ((__cb__ half *)w_col + (axis_k1_outer_db4 * 2560)), 112, 80, 16, (axis_k1_outer_db4 == 0));
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
        for (int32_t lower9 = 0; lower9 < 5; ++lower9) {
          pipe_barrier(PIPE_MTE1);
          img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_4 + (lower9 * 256)), ((__cbuf__ half *)out_diff_local_L1_14 + 0), ((uint64_t)((((((int64_t)((axis_k1_outer_db4 * 2) + 1)) * (int64_t)5) + ((int64_t)lower9)) - ((int64_t)(((uint64_t)(((((int64_t)((axis_k1_outer_db4 * 2) + 1)) * (int64_t)5) + ((int64_t)lower9)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)((axis_k1_outer_db4 * 2) + 1)) * (int64_t)5) + ((int64_t)lower9)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, ((((int64_t)((dx_hw_idx_outer_inner_db1 * 2) + 1)) * (int64_t)4) - (int64_t)4), ((uint64_t)(((((int64_t)((axis_k1_outer_db4 * 2) + 1)) * (int64_t)5) + ((int64_t)lower9)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
        }
        set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        pipe_barrier(PIPE_M);
        wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
        mad(((__cc__ float *)C_2 + 0), ((__ca__ half *)im2col_fractal_4 + 0), ((__cb__ half *)w_col + ((axis_k1_outer_db4 * 2560) + 1280)), 112, 80, 16, (int8_t)0);
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
      }
      set_flag(PIPE_M, PIPE_V, EVENT_ID0);
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
      wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
      copy_matrix_cc_to_ubuf(((__ubuf__ half *)c_ub_7 + 0), ((__cc__ float *)C_2 + 0), 0, 1, 7, 0, 0, CRMODE_F32toF16_NONE);
      set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      set_flag(PIPE_V, PIPE_M, EVENT_ID1);
      wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
      copy_ubuf_to_gm(((__gm__ half *)c_ddr + (((dx_batch_idx_outer_inner_db * 25088) + (dx_hw_idx_outer_inner_db1 * 3584)) + 14336)), ((__ubuf__ half *)c_ub_7 + 0), 0, 1, 112, 0, 0);
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    }
    wait_flag(PIPE_V, PIPE_M, EVENT_ID1);
    wait_flag(PIPE_V, PIPE_M, EVENT_ID0);
    wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID2);
    for (int32_t axis_k1_outer_db5 = 0; axis_k1_outer_db5 < 5; ++axis_k1_outer_db5) {
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      for (int32_t lower10 = 0; lower10 < 5; ++lower10) {
        pipe_barrier(PIPE_MTE1);
        img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_5 + (lower10 * 256)), ((__cbuf__ half *)out_diff_local_L1_14 + 0), ((uint64_t)((((((int64_t)(axis_k1_outer_db5 * 2)) * (int64_t)5) + ((int64_t)lower10)) - ((int64_t)(((uint64_t)(((((int64_t)(axis_k1_outer_db5 * 2)) * (int64_t)5) + ((int64_t)lower10)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)(axis_k1_outer_db5 * 2)) * (int64_t)5) + ((int64_t)lower10)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, (int64_t)20, ((uint64_t)(((((int64_t)(axis_k1_outer_db5 * 2)) * (int64_t)5) + ((int64_t)lower10)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
      }
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      pipe_barrier(PIPE_M);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)C_6 + 0), ((__ca__ half *)im2col_fractal_5 + 0), ((__cb__ half *)w_col + (axis_k1_outer_db5 * 2560)), 112, 80, 16, (axis_k1_outer_db5 == 0));
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
      for (int32_t lower11 = 0; lower11 < 5; ++lower11) {
        pipe_barrier(PIPE_MTE1);
        img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal_8 + (lower11 * 256)), ((__cbuf__ half *)out_diff_local_L1_14 + 0), ((uint64_t)((((((int64_t)((axis_k1_outer_db5 * 2) + 1)) * (int64_t)5) + ((int64_t)lower11)) - ((int64_t)(((uint64_t)(((((int64_t)((axis_k1_outer_db5 * 2) + 1)) * (int64_t)5) + ((int64_t)lower11)) / (int64_t)25)) * (uint64_t)25))) % (int64_t)5)), ((uint64_t)((((((int64_t)((axis_k1_outer_db5 * 2) + 1)) * (int64_t)5) + ((int64_t)lower11)) % (int64_t)25) / (int64_t)5)), (int64_t)-4, (int64_t)20, ((uint64_t)(((((int64_t)((axis_k1_outer_db5 * 2) + 1)) * (int64_t)5) + ((int64_t)lower11)) / (int64_t)25)), (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)5, (uint64_t)1, (uint64_t)1, (uint64_t)5, (uint64_t)1, (uint64_t)7, (csize_t)0);
      }
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      pipe_barrier(PIPE_M);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)C_6 + 0), ((__ca__ half *)im2col_fractal_8 + 0), ((__cb__ half *)w_col + ((axis_k1_outer_db5 * 2560) + 1280)), 112, 80, 16, (int8_t)0);
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
    }
    set_flag(PIPE_M, PIPE_V, EVENT_ID0);
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
    wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    copy_matrix_cc_to_ubuf(((__ubuf__ half *)c_ub_11 + 0), ((__cc__ float *)C_6 + 0), 0, 1, 7, 0, 0, CRMODE_F32toF16_NONE);
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)c_ddr + ((dx_batch_idx_outer_inner_db * 25088) + 23296)), ((__ubuf__ half *)c_ub_11 + 0), 0, 1, 112, 0, 0);
  }
  wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
  wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
  wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
  wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
  pipe_barrier(PIPE_ALL);
}

