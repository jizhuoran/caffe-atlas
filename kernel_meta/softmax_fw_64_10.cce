#ifdef __CCE_KT_TEST__
#define __aicore__ 
#else
#define __aicore__ [aicore]
#endif


#define VERIFY_L2Buffer_OK(l2DataIndex) \
	(0x01 & (((0xFF)&(~l2_in_main)) >> (l2DataIndex)))

extern "C"  __global__ __aicore__ void softmax_fw_64_10__kernel0(__gm__ half* __restrict__ data, __gm__ half* __restrict__ mul_12, __gm__ half* __restrict__ reduce_0, __gm__ half* __restrict__ reduce_1, __gm__ half* __restrict__ broadcast_tensor_1, __gm__ half* __restrict__ exp_2,int64_t index0, uint64_t offset0, int64_t index1, uint64_t offset1, int64_t index2, uint64_t offset2, int64_t index3, uint64_t offset3, int64_t index4, uint64_t offset4, int64_t index5, uint64_t offset5) {
  if (index0 >= 0) {
    if (VERIFY_L2Buffer_OK(index0)) {
      data = (__gm__ half*)((uint64_t)l2_vaddr_base + offset0);
    }
  }
  if (index1 >= 0) {
    if (VERIFY_L2Buffer_OK(index1)) {
      mul_12 = (__gm__ half*)((uint64_t)l2_vaddr_base + offset1);
    }
  }
  if (index2 >= 0) {
    if (VERIFY_L2Buffer_OK(index2)) {
      reduce_0 = (__gm__ half*)((uint64_t)l2_vaddr_base + offset2);
    }
  }
  if (index3 >= 0) {
    if (VERIFY_L2Buffer_OK(index3)) {
      reduce_1 = (__gm__ half*)((uint64_t)l2_vaddr_base + offset3);
    }
  }
  if (index4 >= 0) {
    if (VERIFY_L2Buffer_OK(index4)) {
      broadcast_tensor_1 = (__gm__ half*)((uint64_t)l2_vaddr_base + offset4);
    }
  }
  if (index5 >= 0) {
    if (VERIFY_L2Buffer_OK(index5)) {
      exp_2 = (__gm__ half*)((uint64_t)l2_vaddr_base + offset5);
    }
  }
set_vector_mask((uint64_t)-1, (uint64_t)-1);
__ubuf__   half* data_local_UB = (__ubuf__  half *)get_imm(0);
__ubuf__   half* reduce_0_local_UB = (__ubuf__  half *)get_imm(32);
__ubuf__   half* data_local_UB3 = (__ubuf__  half *)get_imm(0);
__ubuf__   half* reduce_0_local_UB3 = (__ubuf__  half *)get_imm(1280);
__ubuf__   half* broadcast_tensor_0_local_UB3 = (__ubuf__  half *)get_imm(1408);
   half reg_buf[1] = {0};
__ubuf__   half* reduce_1_local_UB2 = (__ubuf__  half *)get_imm(0);
__ubuf__   half* broadcast_tensor_1_local_UB2 = (__ubuf__  half *)get_imm(128);
   half reg_buf1[1] = {0};
__ubuf__   half* broadcast_tensor_1_local_UB4 = (__ubuf__  half *)get_imm(1280);
__ubuf__   half* rec_3_local_UB4 = (__ubuf__  half *)get_imm(2560);
__ubuf__   half* rec_4_local_UB4 = (__ubuf__  half *)get_imm(3840);
  set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
  set_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
  for (int32_t i0 = 0; i0 < 64; ++i0) {
    wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    copy_gm_to_ubuf(((__ubuf__ half *)data_local_UB + 0), ((__gm__ half *)data + (i0 * 10)), 0, 1, 1, 0, 0);
    set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    set_vector_mask((uint64_t)0, (uint64_t)1023);
    wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    vcmax(((__ubuf__ half *)data_local_UB + 0), ((__ubuf__ half *)data_local_UB + 0), 1, 1, 1, 8);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    pipe_barrier(PIPE_V);
    copy_ubuf_to_ubuf(((__ubuf__ half *)reduce_0_local_UB + 0), ((__ubuf__ half *)data_local_UB + 0), 0, 1, 1, 1, 1);
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    set_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    set_vector_mask((uint64_t)18446744073709551615, (uint64_t)18446744073709551615);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)reduce_0 + i0), ((__ubuf__ half *)reduce_0_local_UB + 0), 0, 1, 1, 0, 0);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
  }
  set_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  copy_gm_to_ubuf(((__ubuf__ half *)data_local_UB3 + 0), ((__gm__ half *)data + 0), 0, 1, 40, 0, 0);
  set_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
  copy_gm_to_ubuf(((__ubuf__ half *)reduce_0_local_UB3 + 0), ((__gm__ half *)reduce_0 + 0), 0, 1, 4, 0, 0);
  set_flag(PIPE_MTE2, PIPE_S, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_S, EVENT_ID0);
  for (int32_t i0_c = 0; i0_c < 64; ++i0_c) {
    reg_buf[0] = (half) (*( __ubuf__ half* ) (((__ubuf__ half *)reduce_0_local_UB3 + i0_c)));
    for (int32_t idx = 0; idx < 10; ++idx) {
      (*(__ubuf__ half * )((__ubuf__ half *)broadcast_tensor_0_local_UB3 + ((i0_c * 10) + idx)) )  = reg_buf[0];
    }
  }
  set_flag(PIPE_S, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_S, PIPE_V, EVENT_ID0);
  vsub(((__ubuf__ half *)data_local_UB3 + 0), ((__ubuf__ half *)data_local_UB3 + 0), ((__ubuf__ half *)broadcast_tensor_0_local_UB3 + 0), (uint8_t)5, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vexp(((__ubuf__ half *)data_local_UB3 + 0), ((__ubuf__ half *)data_local_UB3 + 0), (uint8_t)5, (uint16_t)1, (uint16_t)1, (uint8_t)8, (uint8_t)8);
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_gm(((__gm__ half *)exp_2 + 0), ((__ubuf__ half *)data_local_UB3 + 0), 0, 1, 40, 0, 0);
  set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
  set_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  for (int32_t i01 = 0; i01 < 64; ++i01) {
    wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    copy_gm_to_ubuf(((__ubuf__ half *)data_local_UB + 0), ((__gm__ half *)exp_2 + (i01 * 10)), 0, 1, 1, 0, 0);
    set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    set_vector_mask((uint64_t)0, (uint64_t)1023);
    wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    vcadd(((__ubuf__ half *)data_local_UB + 0), ((__ubuf__ half *)data_local_UB + 0), 1, 1, 1, 8);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    pipe_barrier(PIPE_V);
    copy_ubuf_to_ubuf(((__ubuf__ half *)reduce_0_local_UB + 0), ((__ubuf__ half *)data_local_UB + 0), 0, 1, 1, 1, 1);
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    set_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    set_vector_mask((uint64_t)18446744073709551615, (uint64_t)18446744073709551615);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)reduce_1 + i01), ((__ubuf__ half *)reduce_0_local_UB + 0), 0, 1, 1, 0, 0);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
  }
  set_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  copy_gm_to_ubuf(((__ubuf__ half *)reduce_1_local_UB2 + 0), ((__gm__ half *)reduce_1 + 0), 0, 1, 4, 0, 0);
  set_flag(PIPE_MTE2, PIPE_S, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_S, EVENT_ID0);
  for (int32_t i0_c1 = 0; i0_c1 < 64; ++i0_c1) {
    reg_buf1[0] = (half) (*( __ubuf__ half* ) (((__ubuf__ half *)reduce_1_local_UB2 + i0_c1)));
    for (int32_t idx1 = 0; idx1 < 10; ++idx1) {
      (*(__ubuf__ half * )((__ubuf__ half *)broadcast_tensor_1_local_UB2 + ((i0_c1 * 10) + idx1)) )  = reg_buf1[0];
    }
  }
  set_flag(PIPE_S, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_S, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_gm(((__gm__ half *)broadcast_tensor_1 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB2 + 0), 0, 1, 40, 0, 0);
  set_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
  copy_gm_to_ubuf(((__ubuf__ half *)data_local_UB3 + 0), ((__gm__ half *)exp_2 + 0), 0, 1, 40, 0, 0);
  copy_gm_to_ubuf(((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__gm__ half *)broadcast_tensor_1 + 0), 0, 1, 40, 0, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  vrec(((__ubuf__ half *)rec_3_local_UB4 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), (uint8_t)5, (uint16_t)1, (uint16_t)1, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vmul(((__ubuf__ half *)rec_4_local_UB4 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__ubuf__ half *)rec_3_local_UB4 + 0), (uint8_t)5, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vmuls(((__ubuf__ half *)rec_4_local_UB4 + 0), ((__ubuf__ half *)rec_4_local_UB4 + 0), (half)-1.000000e+00f, (uint8_t)5, (uint16_t)1, (uint16_t)1, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vadds(((__ubuf__ half *)rec_4_local_UB4 + 0), ((__ubuf__ half *)rec_4_local_UB4 + 0), (half)2.000000e+00f, (uint8_t)5, (uint16_t)1, (uint16_t)1, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vmul(((__ubuf__ half *)rec_4_local_UB4 + 0), ((__ubuf__ half *)rec_4_local_UB4 + 0), ((__ubuf__ half *)rec_3_local_UB4 + 0), (uint8_t)5, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vmul(((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__ubuf__ half *)rec_4_local_UB4 + 0), (uint8_t)5, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vmuls(((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), (half)-1.000000e+00f, (uint8_t)5, (uint16_t)1, (uint16_t)1, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vadds(((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), (half)2.000000e+00f, (uint8_t)5, (uint16_t)1, (uint16_t)1, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vmul(((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), ((__ubuf__ half *)rec_4_local_UB4 + 0), (uint8_t)5, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  pipe_barrier(PIPE_V);
  vmul(((__ubuf__ half *)data_local_UB3 + 0), ((__ubuf__ half *)data_local_UB3 + 0), ((__ubuf__ half *)broadcast_tensor_1_local_UB4 + 0), (uint8_t)5, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_gm(((__gm__ half *)mul_12 + 0), ((__ubuf__ half *)data_local_UB3 + 0), 0, 1, 40, 0, 0);
  pipe_barrier(PIPE_ALL);
}

