#ifdef __CCE_KT_TEST__
#define __aicore__ 
#else
#define __aicore__ [aicore]
#endif


#define VERIFY_L2Buffer_OK(l2DataIndex) \
	(0x01 & (((0xFF)&(~l2_in_main)) >> (l2DataIndex)))

extern "C"  __global__ __aicore__ void matmul_op_64_16_512_NTA_NTB_nobias__kernel0(__gm__ half* __restrict__ tensor_a, __gm__ half* __restrict__ tensor_b, __gm__ half* __restrict__ tensor_c_gm,int64_t index0, uint64_t offset0, int64_t index1, uint64_t offset1, int64_t index2, uint64_t offset2) {
  if (index0 >= 0) {
    if (VERIFY_L2Buffer_OK(index0)) {
      tensor_a = (__gm__ half*)((uint64_t)l2_vaddr_base + offset0);
    }
  }
  if (index1 >= 0) {
    if (VERIFY_L2Buffer_OK(index1)) {
      tensor_b = (__gm__ half*)((uint64_t)l2_vaddr_base + offset1);
    }
  }
  if (index2 >= 0) {
    if (VERIFY_L2Buffer_OK(index2)) {
      tensor_c_gm = (__gm__ half*)((uint64_t)l2_vaddr_base + offset2);
    }
  }
set_vector_mask((uint64_t)-1, (uint64_t)-1);
__ubuf__   half* tensor_b_ub = (__ubuf__  half *)get_imm(0);
__ubuf__   half* tensor_b_ub_fract = (__ubuf__  half *)get_imm(15872);
__cbuf__   half* tensor_b_l1 = (__cbuf__  half *)get_imm(0);
__ubuf__   half* tensor_a_ub = (__ubuf__  half *)get_imm(31744);
__cbuf__   half* tensor_a_l1 = (__cbuf__  half *)get_imm(15872);
__ca__   half* tensor_a_l0a = (__ca__  half *)get_imm(0);
__cb__   half* tensor_b_l0b = (__cb__  half *)get_imm(0);
__cc__   float* tensor_c = (__cc__  float *)get_imm(0);
__ubuf__   half* tensor_c_ub = (__ubuf__  half *)get_imm(0);
__ubuf__   half* tensor_c_ub_fract = (__ubuf__  half *)get_imm(63488);
__ubuf__   half* tensor_b_ub1 = (__ubuf__  half *)get_imm(126976);
__cbuf__   half* tensor_b_l11 = (__cbuf__  half *)get_imm(0);
__ubuf__   half* tensor_a_ub1 = (__ubuf__  half *)get_imm(127488);
__cbuf__   half* tensor_a_l11 = (__cbuf__  half *)get_imm(512);
__cb__   half* tensor_b_l0b1 = (__cb__  half *)get_imm(0);
__cc__   float* tensor_c1 = (__cc__  float *)get_imm(0);
__ubuf__   half* tensor_c_ub1 = (__ubuf__  half *)get_imm(0);
  copy_gm_to_ubuf(((__ubuf__ half *)tensor_b_ub + 0), ((__gm__ half *)tensor_b + 0), 0, 16, 31, 1, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  set_vector_mask((uint64_t)18446744073709551615, (uint64_t)18446744073709551615);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  for (int32_t iter_cut_axis_1 = 0; iter_cut_axis_1 < 2; ++iter_cut_axis_1) {
    vmax(((__ubuf__ half *)tensor_b_ub_fract + (iter_cut_axis_1 * 128)), ((__ubuf__ half *)tensor_b_ub + (iter_cut_axis_1 * 3968)), ((__ubuf__ half *)tensor_b_ub + (iter_cut_axis_1 * 3968)), (uint8_t)31, (uint8_t)1, (uint8_t)31, (uint8_t)31, (uint8_t)16, (uint8_t)1, (uint8_t)1);
  }
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_b_l1 + 0), ((__ubuf__ half *)tensor_b_ub_fract + 0), 0, 1, 496, 0, 0);
  copy_gm_to_ubuf(((__ubuf__ half *)tensor_a_ub + 0), ((__gm__ half *)tensor_a + 0), 0, 1, 64, 0, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  vmax(((__ubuf__ half *)tensor_a_ub + 0), ((__ubuf__ half *)tensor_a_ub + 0), ((__ubuf__ half *)tensor_a_ub + 0), (uint8_t)8, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_a_l1 + 0), ((__ubuf__ half *)tensor_a_ub + 0), 0, 1, 64, 0, 0);
  set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
  load_cbuf_to_ca(((__ca__ half *)tensor_a_l0a + 0), ((__cbuf__ half *)tensor_a_l1 + 0), 0, 4, 1, 0, 0);
  load_cbuf_to_cb(((__cb__ half *)tensor_b_l0b + 0), ((__cbuf__ half *)tensor_b_l1 + 0), 0, 31, 1, 0, 1);
  set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
  wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
  mad(((__cc__ float *)tensor_c + 0), ((__ca__ half *)tensor_a_l0a + 0), ((__cb__ half *)tensor_b_l0b + 0), 64, 16, 496, (int8_t)1);
  set_flag(PIPE_M, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
  copy_matrix_cc_to_ubuf(((__ubuf__ half *)tensor_c_ub + 0), ((__cc__ float *)tensor_c + 0), 0, 1, 124, 0, 0, CRMODE_F32toF16_NONE);
  pipe_barrier(PIPE_V);
  for (int32_t iter_cut_axis_11 = 0; iter_cut_axis_11 < 8; ++iter_cut_axis_11) {
    vmax(((__ubuf__ half *)tensor_c_ub_fract + (iter_cut_axis_11 * 3968)), ((__ubuf__ half *)tensor_c_ub + (iter_cut_axis_11 * 128)), ((__ubuf__ half *)tensor_c_ub + (iter_cut_axis_11 * 128)), (uint8_t)31, (uint8_t)31, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)64, (uint8_t)64);
  }
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_gm(((__gm__ half *)tensor_c_gm + 0), ((__ubuf__ half *)tensor_c_ub_fract + 0), 0, 64, 31, 0, 1);
  copy_gm_to_ubuf(((__ubuf__ half *)tensor_b_ub1 + 0), ((__gm__ half *)tensor_b + 496), 0, 16, 1, 31, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  vmax(((__ubuf__ half *)tensor_b_ub1 + 0), ((__ubuf__ half *)tensor_b_ub1 + 0), ((__ubuf__ half *)tensor_b_ub1 + 0), (uint8_t)2, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_b_l11 + 0), ((__ubuf__ half *)tensor_b_ub1 + 0), 0, 1, 16, 0, 0);
  copy_gm_to_ubuf(((__ubuf__ half *)tensor_a_ub1 + 0), ((__gm__ half *)tensor_a + 0), 0, 1, 64, 0, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  vmax(((__ubuf__ half *)tensor_a_ub1 + 0), ((__ubuf__ half *)tensor_a_ub1 + 0), ((__ubuf__ half *)tensor_a_ub1 + 0), (uint8_t)8, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_a_l11 + 0), ((__ubuf__ half *)tensor_a_ub1 + 0), 0, 1, 64, 0, 0);
  set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
  load_cbuf_to_ca(((__ca__ half *)tensor_a_l0a + 0), ((__cbuf__ half *)tensor_a_l11 + 0), 0, 4, 1, 0, 0);
  load_cbuf_to_cb(((__cb__ half *)tensor_b_l0b1 + 0), ((__cbuf__ half *)tensor_b_l11 + 0), 0, 1, 0, 0, 1);
  set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
  wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
  mad(((__cc__ float *)tensor_c1 + 0), ((__ca__ half *)tensor_a_l0a + 0), ((__cb__ half *)tensor_b_l0b1 + 0), 64, 16, 16, (int8_t)1);
  set_flag(PIPE_M, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
  copy_matrix_cc_to_ubuf(((__ubuf__ half *)tensor_c_ub1 + 0), ((__cc__ float *)tensor_c1 + 0), 0, 1, 4, 0, 0, CRMODE_F32toF16_NONE);
  pipe_barrier(PIPE_V);
  vmax(((__ubuf__ half *)tensor_c_ub1 + 0), ((__ubuf__ half *)tensor_c_ub1 + 0), ((__ubuf__ half *)tensor_c_ub1 + 0), (uint8_t)8, (uint8_t)1, (uint8_t)1, (uint8_t)1, (uint8_t)8, (uint8_t)8, (uint8_t)8);
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_gm(((__gm__ half *)tensor_c_gm + 496), ((__ubuf__ half *)tensor_c_ub1 + 0), 0, 64, 1, 0, 31);
  pipe_barrier(PIPE_ALL);
}

