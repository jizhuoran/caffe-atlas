#ifdef __CCE_KT_TEST__
#define __aicore__ 
#else
#define __aicore__ [aicore]
#endif

extern "C"  __global__ __aicore__ void matmul_op_64_512_16_NTA_TB_bias__kernel0(__gm__ half* __restrict__ tensor_a, __gm__ half* __restrict__ tensor_b, __gm__ half* __restrict__ tensor_blas, __gm__ half* __restrict__ tensor_c_gm) {
set_vector_mask((uint64_t)-1, (uint64_t)-1);
set_ctrl(sbitset0(get_ctrl(), 56));
__ubuf__   half* tensor_a_ub = (__ubuf__  half *)get_imm(0);
__ubuf__   half* tensor_a_ub_fract = (__ubuf__  half *)get_imm(32768);
__cbuf__   half* tensor_a_l1 = (__cbuf__  half *)get_imm(0);
__ubuf__   half* tensor_b_ub = (__ubuf__  half *)get_imm(65536);
__ubuf__   half* tensor_b_ub_fract = (__ubuf__  half *)get_imm(81920);
__cbuf__   half* tensor_b_l1 = (__cbuf__  half *)get_imm(32768);
__ubuf__   half* tensor_bias_ub = (__ubuf__  half *)get_imm(98304);
__cc__   float* tensor_bias_l0c = (__cc__  float *)get_imm(0);
__ca__   half* tensor_a_l0a = (__ca__  half *)get_imm(0);
__cb__   half* tensor_b_l0b = (__cb__  half *)get_imm(0);
__ubuf__   half* tensor_c_ub = (__ubuf__  half *)get_imm(0);
  copy_gm_to_ubuf(((__ubuf__ half *)tensor_a_ub), ((__gm__ half *)tensor_a + (((int32_t)block_idx) * 16384)), 0, 1, 1024, 0, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  set_vector_mask(0xffffffffffffffff, 0xffffffffffffffff);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  for (int32_t iter_lower_outer_0 = 0; iter_lower_outer_0 < 2; ++iter_lower_outer_0) {
    for (int32_t iter_cut_axis_1 = 0; iter_cut_axis_1 < 2; ++iter_cut_axis_1) {
      vmax(((__ubuf__ half *)tensor_a_ub_fract + ((iter_lower_outer_0 * 8192) + (iter_cut_axis_1 * 128))), ((__ubuf__ half *)tensor_a_ub + ((iter_lower_outer_0 * 8192) + (iter_cut_axis_1 * 4096))), ((__ubuf__ half *)tensor_a_ub + ((iter_lower_outer_0 * 8192) + (iter_cut_axis_1 * 4096))), (uint8_t)32ULL, (uint8_t)1ULL, (uint8_t)32ULL, (uint8_t)32ULL, (uint8_t)16ULL, (uint8_t)1ULL, (uint8_t)1ULL);
    }
  }
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_a_l1), ((__ubuf__ half *)tensor_a_ub_fract), 0, 1, 1024, 0, 0);
  set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
  copy_gm_to_ubuf(((__ubuf__ half *)tensor_b_ub), ((__gm__ half *)tensor_b), 0, 1, 512, 0, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  for (int32_t iter_cut_axis_11 = 0; iter_cut_axis_11 < 2; ++iter_cut_axis_11) {
    vmax(((__ubuf__ half *)tensor_b_ub_fract + (iter_cut_axis_11 * 128)), ((__ubuf__ half *)tensor_b_ub + (iter_cut_axis_11 * 4096)), ((__ubuf__ half *)tensor_b_ub + (iter_cut_axis_11 * 4096)), (uint8_t)32ULL, (uint8_t)1ULL, (uint8_t)32ULL, (uint8_t)32ULL, (uint8_t)16ULL, (uint8_t)1ULL, (uint8_t)1ULL);
  }
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_b_l1), ((__ubuf__ half *)tensor_b_ub_fract), 0, 1, 512, 0, 0);
  set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID1);
  copy_gm_to_ubuf(((__ubuf__ half *)tensor_bias_ub), ((__gm__ half *)tensor_blas), 0, 1, 1, 0, 0);
  set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
  for (int32_t brc_part = 0; brc_part < 2; ++brc_part) {
    broadcast_ub_to_cc(((__cc__ float *)tensor_bias_l0c + (brc_part * 256)), ((__ubuf__ half *)tensor_bias_ub), 1, 1, 0, 0);
  }
  set_flag(PIPE_V, PIPE_M, EVENT_ID0);
  wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
  load_cbuf_to_ca(((__ca__ half *)tensor_a_l0a), ((__cbuf__ half *)tensor_a_l1), 0, 64, 1, 0, 0);
  wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID1);
  load_cbuf_to_cb(((__cb__ half *)tensor_b_l0b), ((__cbuf__ half *)tensor_b_l1), 0, 32, 1, 0, 0);
  set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_M, EVENT_ID0);
  wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
  mad(((__cc__ float *)tensor_bias_l0c), ((__ca__ half *)tensor_a_l0a), ((__cb__ half *)tensor_b_l0b), 32, 512, 16, (int8_t)0ULL);
  set_flag(PIPE_M, PIPE_V, EVENT_ID0);
    wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
  copy_matrix_cc_to_ubuf(((__ubuf__ half *)tensor_c_ub), ((__cc__ float *)tensor_bias_l0c), 0, 1, 2, 0, 0, CRMODE_F32toF16_NONE);
  pipe_barrier(PIPE_V);
  vmax(((__ubuf__ half *)tensor_c_ub), ((__ubuf__ half *)tensor_c_ub), ((__ubuf__ half *)tensor_c_ub), (uint8_t)4ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)8ULL, (uint8_t)8ULL, (uint8_t)8ULL);
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_gm(((__gm__ half *)tensor_c_gm + (((int32_t)block_idx) * 512)), ((__ubuf__ half *)tensor_c_ub), 0, 1, 32, 0, 0);
  pipe_barrier(PIPE_ALL);
}

