#ifdef __CCE_KT_TEST__
#define __aicore__ 
#else
#define __aicore__ [aicore]
#endif

extern "C"  __global__ __aicore__ void conv_fw_op_64_20_50_12_12_bias_5_5_0_0_1_1__kernel0(__gm__ half* __restrict__ Fmap, __gm__ half* __restrict__ Filter, __gm__ half* __restrict__ bias_tensor, __gm__ half* __restrict__ remove_pad_cc_4) {
set_l1_3d_size(0);
set_padding(0);
set_vector_mask((uint64_t)-1, (uint64_t)-1);
set_ctrl(sbitset0(get_ctrl(), 56));
__cbuf__   half* Filter_local_L1 = (__cbuf__  half *)get_imm(0);
__ubuf__   half* bias_tensor_local_UB = (__ubuf__  half *)get_imm(0);
__cbuf__   half* Fmap_local_L1_1 = (__cbuf__  half *)get_imm(102400);
__cc__   float* mad1 = (__cc__  float *)get_imm(0);
__cb__   half* Filter_local_L1_local_L0B = (__cb__  half *)get_imm(0);
__ca__   half* im2col_fractal = (__ca__  half *)get_imm(0);
__ubuf__   half* C_UB = (__ubuf__  half *)get_imm(128);
__cc__   float* mad11 = (__cc__  float *)get_imm(12288);
__ca__   half* im2col_fractal1 = (__ca__  half *)get_imm(38400);
__ubuf__   half* C_UB1 = (__ubuf__  half *)get_imm(6272);
__cbuf__   half* Fmap_local_L1_2 = (__cbuf__  half *)get_imm(111616);
__ubuf__   half* C_UB2 = (__ubuf__  half *)get_imm(8320);
__ubuf__   half* C_UB3 = (__ubuf__  half *)get_imm(14464);
  set_fmatrix(0xc000c);
  copy_gm_to_cbuf(((__cbuf__ half *)Filter_local_L1), ((__gm__ half *)Filter), 0, 1, 3200, 0, 0, PAD_NONE);
  copy_gm_to_ubuf(((__ubuf__ half *)bias_tensor_local_UB), ((__gm__ half *)bias_tensor), 0, 1, 4, 0, 0);
  set_vector_mask(0xffffffffffffffff, 0xffffffffffffffff);
  for (int32_t i0_inner_outer_outer_db = 0; i0_inner_outer_outer_db < 16; ++i0_inner_outer_outer_db) {
    if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
    }
    copy_gm_to_cbuf(((__cbuf__ half *)Fmap_local_L1_1), ((__gm__ half *)Fmap + ((((int32_t)block_idx) * 147456) + (i0_inner_outer_outer_db * 9216))), 0, 1, 288, 0, 0, PAD_NONE);
    set_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
        if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_V, PIPE_M, EVENT_ID0);
    }
    wait_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
    for (int32_t k1_outer_inner = 0; k1_outer_inner < 2; ++k1_outer_inner) {
      if ((k1_outer_inner + i0_inner_outer_outer_db) != 0) {
        wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      }
      load_cbuf_to_cb(((__cb__ half *)Filter_local_L1_local_L0B), ((__cbuf__ half *)Filter_local_L1 + (k1_outer_inner * 25600)), 0, 100, 1, 0, 0);
      for (int32_t lower = 0; lower < 3; ++lower) {
        img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal + (lower * 6400)), ((__cbuf__ half *)Fmap_local_L1_1), ((uint64_t)(((((int64_t)k1_outer_inner) * (int64_t)25) - ((int64_t)(((uint64_t)((int64_t)k1_outer_inner)) * (uint64_t)25ULL))) % (int64_t)5)), (uint64_t)0ULL, (int64_t)0, (((int64_t)lower) * (int64_t)2), ((uint64_t)((int64_t)k1_outer_inner)), (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)5ULL, (uint64_t)5ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)0ULL, (uint64_t)25ULL, CSIZE0);
      }
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)mad1), ((__ca__ half *)im2col_fractal), ((__cb__ half *)Filter_local_L1_local_L0B), 48, 400, 64, (k1_outer_inner == 0));
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    }
    set_flag(PIPE_M, PIPE_V, EVENT_ID0);
    if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    }
    wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
    copy_matrix_cc_to_ubuf(((__ubuf__ half *)C_UB), ((__cc__ float *)mad1), 0, 1, 12, 0, 0, CRMODE_F32toF16_NONE);
    set_flag(PIPE_V, PIPE_M, EVENT_ID0);
    pipe_barrier(PIPE_V);
    for (int32_t iter_cut_axis_1 = 0; iter_cut_axis_1 < 4; ++iter_cut_axis_1) {
      vadd(((__ubuf__ half *)C_UB + (iter_cut_axis_1 * 768)), ((__ubuf__ half *)C_UB + (iter_cut_axis_1 * 768)), ((__ubuf__ half *)bias_tensor_local_UB + (iter_cut_axis_1 * 16)), (uint8_t)6ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)0ULL, (uint8_t)8ULL, (uint8_t)8ULL, (uint8_t)0ULL);
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)remove_pad_cc_4 + ((((int32_t)block_idx) * 131072) + (i0_inner_outer_outer_db * 8192))), ((__ubuf__ half *)C_UB), 0, 4, 48, 0, 16);
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    }
    if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_V, PIPE_M, EVENT_ID1);
    }
    for (int32_t k1_outer_inner1 = 0; k1_outer_inner1 < 2; ++k1_outer_inner1) {
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      load_cbuf_to_cb(((__cb__ half *)Filter_local_L1_local_L0B), ((__cbuf__ half *)Filter_local_L1 + (k1_outer_inner1 * 25600)), 0, 100, 1, 0, 0);
      img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal1), ((__cbuf__ half *)Fmap_local_L1_1), ((uint64_t)(((((int64_t)k1_outer_inner1) * (int64_t)25) - ((int64_t)(((uint64_t)((int64_t)k1_outer_inner1)) * (uint64_t)25ULL))) % (int64_t)5)), (uint64_t)0ULL, (int64_t)0, (int64_t)6, ((uint64_t)((int64_t)k1_outer_inner1)), (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)5ULL, (uint64_t)5ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)0ULL, (uint64_t)25ULL, CSIZE0);
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)mad11), ((__ca__ half *)im2col_fractal1), ((__cb__ half *)Filter_local_L1_local_L0B), 16, 400, 64, (k1_outer_inner1 == 0));
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    }
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
    }
    set_flag(PIPE_M, PIPE_V, EVENT_ID0);
    if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    }
    wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
    copy_matrix_cc_to_ubuf(((__ubuf__ half *)C_UB1), ((__cc__ float *)mad11), 0, 1, 4, 0, 0, CRMODE_F32toF16_NONE);
    set_flag(PIPE_V, PIPE_M, EVENT_ID1);
    pipe_barrier(PIPE_V);
    for (int32_t iter_cut_axis_11 = 0; iter_cut_axis_11 < 2; ++iter_cut_axis_11) {
      vadd(((__ubuf__ half *)C_UB1 + (iter_cut_axis_11 * 128)), ((__ubuf__ half *)C_UB1 + (iter_cut_axis_11 * 128)), ((__ubuf__ half *)bias_tensor_local_UB), (uint8_t)4ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)0ULL, (uint8_t)16ULL, (uint8_t)16ULL, (uint8_t)1ULL);
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)remove_pad_cc_4 + (((((int32_t)block_idx) * 131072) + (i0_inner_outer_outer_db * 8192)) + 768)), ((__ubuf__ half *)C_UB1), 0, 4, 16, 0, 48);
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    }
    if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
    }
    copy_gm_to_cbuf(((__cbuf__ half *)Fmap_local_L1_2), ((__gm__ half *)Fmap + (((((int32_t)block_idx) * 147456) + (i0_inner_outer_outer_db * 9216)) + 4608)), 0, 1, 288, 0, 0, PAD_NONE);
    set_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
        wait_flag(PIPE_V, PIPE_M, EVENT_ID0);
    wait_flag(PIPE_MTE2, PIPE_MTE1, EVENT_ID0);
    for (int32_t k1_outer_inner2 = 0; k1_outer_inner2 < 2; ++k1_outer_inner2) {
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      load_cbuf_to_cb(((__cb__ half *)Filter_local_L1_local_L0B), ((__cbuf__ half *)Filter_local_L1 + (k1_outer_inner2 * 25600)), 0, 100, 1, 0, 0);
      for (int32_t lower1 = 0; lower1 < 3; ++lower1) {
        img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal + (lower1 * 6400)), ((__cbuf__ half *)Fmap_local_L1_2), ((uint64_t)(((((int64_t)k1_outer_inner2) * (int64_t)25) - ((int64_t)(((uint64_t)((int64_t)k1_outer_inner2)) * (uint64_t)25ULL))) % (int64_t)5)), (uint64_t)0ULL, (int64_t)0, (((int64_t)lower1) * (int64_t)2), ((uint64_t)((int64_t)k1_outer_inner2)), (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)5ULL, (uint64_t)5ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)0ULL, (uint64_t)25ULL, CSIZE0);
      }
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)mad1), ((__ca__ half *)im2col_fractal), ((__cb__ half *)Filter_local_L1_local_L0B), 48, 400, 64, (k1_outer_inner2 == 0));
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    }
    set_flag(PIPE_M, PIPE_V, EVENT_ID0);
    if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID2);
    }
    wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
    copy_matrix_cc_to_ubuf(((__ubuf__ half *)C_UB2), ((__cc__ float *)mad1), 0, 1, 12, 0, 0, CRMODE_F32toF16_NONE);
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_V, PIPE_M, EVENT_ID0);
    }
    pipe_barrier(PIPE_V);
    for (int32_t iter_cut_axis_12 = 0; iter_cut_axis_12 < 4; ++iter_cut_axis_12) {
      vadd(((__ubuf__ half *)C_UB2 + (iter_cut_axis_12 * 768)), ((__ubuf__ half *)C_UB2 + (iter_cut_axis_12 * 768)), ((__ubuf__ half *)bias_tensor_local_UB + (iter_cut_axis_12 * 16)), (uint8_t)6ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)0ULL, (uint8_t)8ULL, (uint8_t)8ULL, (uint8_t)0ULL);
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)remove_pad_cc_4 + (((((int32_t)block_idx) * 131072) + (i0_inner_outer_outer_db * 8192)) + 4096)), ((__ubuf__ half *)C_UB2), 0, 4, 48, 0, 16);
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID2);
    }
    wait_flag(PIPE_V, PIPE_M, EVENT_ID1);
    for (int32_t k1_outer_inner3 = 0; k1_outer_inner3 < 2; ++k1_outer_inner3) {
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      load_cbuf_to_cb(((__cb__ half *)Filter_local_L1_local_L0B), ((__cbuf__ half *)Filter_local_L1 + (k1_outer_inner3 * 25600)), 0, 100, 1, 0, 0);
      img2col_cbuf_to_ca(((__ca__ half *)im2col_fractal1), ((__cbuf__ half *)Fmap_local_L1_2), ((uint64_t)(((((int64_t)k1_outer_inner3) * (int64_t)25) - ((int64_t)(((uint64_t)((int64_t)k1_outer_inner3)) * (uint64_t)25ULL))) % (int64_t)5)), (uint64_t)0ULL, (int64_t)0, (int64_t)6, ((uint64_t)((int64_t)k1_outer_inner3)), (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)5ULL, (uint64_t)5ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)1ULL, (uint64_t)0ULL, (uint64_t)25ULL, CSIZE0);
      set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
      mad(((__cc__ float *)mad11), ((__ca__ half *)im2col_fractal1), ((__cb__ half *)Filter_local_L1_local_L0B), 16, 400, 64, (k1_outer_inner3 == 0));
      if ((k1_outer_inner3 + i0_inner_outer_outer_db) != 16) {
        set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
      }
    }
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
    }
    set_flag(PIPE_M, PIPE_V, EVENT_ID0);
    if (0 < i0_inner_outer_outer_db) {
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID3);
    }
    wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
    copy_matrix_cc_to_ubuf(((__ubuf__ half *)C_UB3), ((__cc__ float *)mad11), 0, 1, 4, 0, 0, CRMODE_F32toF16_NONE);
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_V, PIPE_M, EVENT_ID1);
    }
    pipe_barrier(PIPE_V);
    for (int32_t iter_cut_axis_13 = 0; iter_cut_axis_13 < 2; ++iter_cut_axis_13) {
      vadd(((__ubuf__ half *)C_UB3 + (iter_cut_axis_13 * 128)), ((__ubuf__ half *)C_UB3 + (iter_cut_axis_13 * 128)), ((__ubuf__ half *)bias_tensor_local_UB), (uint8_t)4ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)0ULL, (uint8_t)16ULL, (uint8_t)16ULL, (uint8_t)1ULL);
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_gm(((__gm__ half *)remove_pad_cc_4 + (((((int32_t)block_idx) * 131072) + (i0_inner_outer_outer_db * 8192)) + 4864)), ((__ubuf__ half *)C_UB3), 0, 4, 16, 0, 48);
    if (i0_inner_outer_outer_db < 15) {
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID3);
    }
  }
  pipe_barrier(PIPE_ALL);
}

