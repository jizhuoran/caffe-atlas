#ifdef __CCE_KT_TEST__
#define __aicore__ 
#else
#define __aicore__ [aicore]
#endif

extern "C"  __global__ __aicore__ void matmul_op_64_512_800_NTA_NTB_nobias__kernel0(__gm__ half* __restrict__ tensor_a, __gm__ half* __restrict__ tensor_b, __gm__ half* __restrict__ tensor_c_gm) {
set_vector_mask((uint64_t)-1, (uint64_t)-1);
set_ctrl(sbitset0(get_ctrl(), 56));
__cc__   float* tensor_c = (__cc__  float *)get_imm(0);
__ubuf__   half* tensor_a_ub_1 = (__ubuf__  half *)get_imm(0);
__ubuf__   half* tensor_a_ub_fract_2 = (__ubuf__  half *)get_imm(4096);
__cbuf__   half* tensor_a_l1_3 = (__cbuf__  half *)get_imm(0);
__ubuf__   half* tensor_b_ub_4 = (__ubuf__  half *)get_imm(8192);
__ubuf__   half* tensor_b_ub_fract_5 = (__ubuf__  half *)get_imm(33792);
__cbuf__   half* tensor_b_l1_6 = (__cbuf__  half *)get_imm(4096);
__ca__   half* tensor_a_l0a_7 = (__ca__  half *)get_imm(0);
__cb__   half* tensor_b_l0b_8 = (__cb__  half *)get_imm(0);
__ubuf__   half* tensor_a_ub_9 = (__ubuf__  half *)get_imm(59392);
__ubuf__   half* tensor_b_ub_12 = (__ubuf__  half *)get_imm(63488);
__ca__   half* tensor_a_l0a_15 = (__ca__  half *)get_imm(4096);
__cb__   half* tensor_b_l0b_16 = (__cb__  half *)get_imm(25600);
__ubuf__   half* tensor_c_ub = (__ubuf__  half *)get_imm(0);
__ubuf__   half* tensor_c_ub_fract = (__ubuf__  half *)get_imm(51200);
  set_vector_mask(0xffffffffffffffff, 0xffffffffffffffff);
  for (int32_t kb_outer_db = 0; kb_outer_db < 8; ++kb_outer_db) {
    if (0 < kb_outer_db) {
      wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    }
    copy_gm_to_ubuf(((__ubuf__ half *)tensor_a_ub_1), ((__gm__ half *)tensor_a + (kb_outer_db * 64)), 0, 64, 2, 30, 0);
    set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    if (0 < kb_outer_db) {
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    }
    wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    for (int32_t iter_lower_outer_0 = 0; iter_lower_outer_0 < 4; ++iter_lower_outer_0) {
      for (int32_t iter_cut_axis_1 = 0; iter_cut_axis_1 < 2; ++iter_cut_axis_1) {
        vmax(((__ubuf__ half *)tensor_a_ub_fract_2 + ((iter_lower_outer_0 * 512) + (iter_cut_axis_1 * 128))), ((__ubuf__ half *)tensor_a_ub_1 + ((iter_lower_outer_0 * 512) + (iter_cut_axis_1 * 256))), ((__ubuf__ half *)tensor_a_ub_1 + ((iter_lower_outer_0 * 512) + (iter_cut_axis_1 * 256))), (uint8_t)2ULL, (uint8_t)1ULL, (uint8_t)2ULL, (uint8_t)2ULL, (uint8_t)16ULL, (uint8_t)1ULL, (uint8_t)1ULL);
      }
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    if (kb_outer_db < 7) {
      set_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    }
    if (0 < kb_outer_db) {
      wait_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID0);
    }
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_a_l1_3), ((__ubuf__ half *)tensor_a_ub_fract_2), 0, 1, 128, 0, 0);
    set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    if (0 < kb_outer_db) {
      wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID1);
    }
    copy_gm_to_ubuf(((__ubuf__ half *)tensor_b_ub_4), ((__gm__ half *)tensor_b + ((kb_outer_db * 51200) + (((int32_t)block_idx) * 400))), 0, 32, 25, 25, 0);
    set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    if (0 < kb_outer_db) {
      wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    }
    wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    for (int32_t iter_lower_outer_01 = 0; iter_lower_outer_01 < 2; ++iter_lower_outer_01) {
      for (int32_t iter_cut_axis_11 = 0; iter_cut_axis_11 < 2; ++iter_cut_axis_11) {
        vmax(((__ubuf__ half *)tensor_b_ub_fract_5 + ((iter_lower_outer_01 * 6400) + (iter_cut_axis_11 * 128))), ((__ubuf__ half *)tensor_b_ub_4 + ((iter_lower_outer_01 * 6400) + (iter_cut_axis_11 * 3200))), ((__ubuf__ half *)tensor_b_ub_4 + ((iter_lower_outer_01 * 6400) + (iter_cut_axis_11 * 3200))), (uint8_t)25ULL, (uint8_t)1ULL, (uint8_t)25ULL, (uint8_t)25ULL, (uint8_t)16ULL, (uint8_t)1ULL, (uint8_t)1ULL);
      }
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    if (kb_outer_db < 7) {
      set_flag(PIPE_V, PIPE_MTE2, EVENT_ID1);
    }
    if (0 < kb_outer_db) {
      wait_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID1);
    }
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_b_l1_6), ((__ubuf__ half *)tensor_b_ub_fract_5), 0, 1, 800, 0, 0);
    set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID1);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    if (0 < kb_outer_db) {
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    }
    wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
    load_cbuf_to_ca(((__ca__ half *)tensor_a_l0a_7), ((__cbuf__ half *)tensor_a_l1_3), 0, 8, 1, 0, 0);
    set_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID0);
    wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID1);
    load_cbuf_to_cb(((__cb__ half *)tensor_b_l0b_8), ((__cbuf__ half *)tensor_b_l1_6), 0, 50, 1, 0, 1);
    set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
    set_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID1);
    wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
    mad(((__cc__ float *)tensor_c), ((__ca__ half *)tensor_a_l0a_7), ((__cb__ half *)tensor_b_l0b_8), 64, 32, 400, (kb_outer_db == 0));
    if (kb_outer_db < 7) {
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    }
    if (0 < kb_outer_db) {
      wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID2);
    }
    copy_gm_to_ubuf(((__ubuf__ half *)tensor_a_ub_9), ((__gm__ half *)tensor_a + ((kb_outer_db * 64) + 32)), 0, 64, 2, 30, 0);
    set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    for (int32_t iter_lower_outer_02 = 0; iter_lower_outer_02 < 4; ++iter_lower_outer_02) {
      for (int32_t iter_cut_axis_12 = 0; iter_cut_axis_12 < 2; ++iter_cut_axis_12) {
        vmax(((__ubuf__ half *)tensor_a_ub_fract_2 + ((iter_lower_outer_02 * 512) + (iter_cut_axis_12 * 128))), ((__ubuf__ half *)tensor_a_ub_9 + ((iter_lower_outer_02 * 512) + (iter_cut_axis_12 * 256))), ((__ubuf__ half *)tensor_a_ub_9 + ((iter_lower_outer_02 * 512) + (iter_cut_axis_12 * 256))), (uint8_t)2ULL, (uint8_t)1ULL, (uint8_t)2ULL, (uint8_t)2ULL, (uint8_t)16ULL, (uint8_t)1ULL, (uint8_t)1ULL);
      }
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    if (kb_outer_db < 7) {
      set_flag(PIPE_V, PIPE_MTE2, EVENT_ID2);
    }
    wait_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID0);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_a_l1_3), ((__ubuf__ half *)tensor_a_ub_fract_2), 0, 1, 128, 0, 0);
    set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
    if (kb_outer_db < 7) {
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    }
    if (0 < kb_outer_db) {
      wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID3);
    }
    copy_gm_to_ubuf(((__ubuf__ half *)tensor_b_ub_12), ((__gm__ half *)tensor_b + (((kb_outer_db * 51200) + (((int32_t)block_idx) * 400)) + 25600)), 0, 32, 25, 25, 0);
    set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
    for (int32_t iter_lower_outer_03 = 0; iter_lower_outer_03 < 2; ++iter_lower_outer_03) {
      for (int32_t iter_cut_axis_13 = 0; iter_cut_axis_13 < 2; ++iter_cut_axis_13) {
        vmax(((__ubuf__ half *)tensor_b_ub_fract_5 + ((iter_lower_outer_03 * 6400) + (iter_cut_axis_13 * 128))), ((__ubuf__ half *)tensor_b_ub_12 + ((iter_lower_outer_03 * 6400) + (iter_cut_axis_13 * 3200))), ((__ubuf__ half *)tensor_b_ub_12 + ((iter_lower_outer_03 * 6400) + (iter_cut_axis_13 * 3200))), (uint8_t)25ULL, (uint8_t)1ULL, (uint8_t)25ULL, (uint8_t)25ULL, (uint8_t)16ULL, (uint8_t)1ULL, (uint8_t)1ULL);
      }
    }
    set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    if (kb_outer_db < 7) {
      set_flag(PIPE_V, PIPE_MTE2, EVENT_ID3);
    }
    wait_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID1);
    wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
    copy_ubuf_to_cbuf(((__cbuf__ half *)tensor_b_l1_6), ((__ubuf__ half *)tensor_b_ub_fract_5), 0, 1, 800, 0, 0);
    set_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID1);
    if (kb_outer_db < 7) {
      set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    }
    if (0 < kb_outer_db) {
      wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
    }
    wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID0);
    load_cbuf_to_ca(((__ca__ half *)tensor_a_l0a_15), ((__cbuf__ half *)tensor_a_l1_3), 0, 8, 1, 0, 0);
    if (kb_outer_db < 7) {
      set_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID0);
    }
    wait_flag(PIPE_MTE3, PIPE_MTE1, EVENT_ID1);
    load_cbuf_to_cb(((__cb__ half *)tensor_b_l0b_16), ((__cbuf__ half *)tensor_b_l1_6), 0, 50, 1, 0, 1);
    set_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
    if (kb_outer_db < 7) {
      set_flag(PIPE_MTE1, PIPE_MTE3, EVENT_ID1);
    }
    wait_flag(PIPE_MTE1, PIPE_M, EVENT_ID0);
    mad(((__cc__ float *)tensor_c), ((__ca__ half *)tensor_a_l0a_15), ((__cb__ half *)tensor_b_l0b_16), 64, 32, 400, (int8_t)0ULL);
    if (kb_outer_db < 7) {
      set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
    }
  }
  set_flag(PIPE_M, PIPE_V, EVENT_ID0);
  wait_flag(PIPE_M, PIPE_V, EVENT_ID0);
  copy_matrix_cc_to_ubuf(((__ubuf__ half *)tensor_c_ub), ((__cc__ float *)tensor_c), 0, 1, 100, 0, 0, CRMODE_F32toF16_NONE);
  pipe_barrier(PIPE_V);
  for (int32_t iter_lower_outer_04 = 0; iter_lower_outer_04 < 4; ++iter_lower_outer_04) {
    for (int32_t iter_cut_axis_14 = 0; iter_cut_axis_14 < 2; ++iter_cut_axis_14) {
      vmax(((__ubuf__ half *)tensor_c_ub_fract + ((iter_lower_outer_04 * 6400) + (iter_cut_axis_14 * 3200))), ((__ubuf__ half *)tensor_c_ub + ((iter_lower_outer_04 * 256) + (iter_cut_axis_14 * 128))), ((__ubuf__ half *)tensor_c_ub + ((iter_lower_outer_04 * 256) + (iter_cut_axis_14 * 128))), (uint8_t)25ULL, (uint8_t)25ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)1ULL, (uint8_t)64ULL, (uint8_t)64ULL);
    }
  }
  set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
  copy_ubuf_to_gm(((__gm__ half *)tensor_c_gm + (((int32_t)block_idx) * 400)), ((__ubuf__ half *)tensor_c_ub_fract), 0, 64, 25, 0, 25);
  pipe_barrier(PIPE_ALL);
}

